[{"content":"A couple of months ago, I came across a video where the prolific streamer and Netflix dev known online as the ThePrimeagen live-coded a simple lexer for an interpreter inspired by the book Crafting Interpreters by Robert Nystrom. If you\u0026rsquo;ve ever looked into what CPython actually does with your source code, you\u0026rsquo;ll know that it parses the code into an abstract syntax tree, converts the tree into bytecode instructions (i.e. those *.pyc files usually in a __pycache__ directory) and runs them on a virtual machine. But before it can do any of that, it has to \u0026ldquo;lex\u0026rdquo; the raw text of the file into meaningful tokens like def, True, etc. I was struck by the elegance of ThePrimeagen\u0026rsquo;s implementation in Rust, and so I set myself a task: how close can I get to the Rust version using Python?1\n(Sort of) Emulating Rust Enums in Python Even a superficial glance at the Rust lexer will give you an appreciation for Rust enums and pattern matching. In Rust, enums can contain data, so enumerating each of the token types that the lexer recognizes is straightforward:\npub enum Token { Ident(String), Int(String), Illegal, Eof, Assign, Bang, Dash, ... Return, True, False, } In Python, it\u0026rsquo;s similarly eas\u0026ndash; uh, oh.\nPython enums just don\u0026rsquo;t work this way. Enum subclass members are expected to be singletons, so we\u0026rsquo;ll have to use a different abstraction. After some finagling, I landed on namedtuples as a suitable alternative.\nfrom collections import namedtuple class Token(TokenBase): Ident = namedtuple(\u0026#34;Ident\u0026#34;, [\u0026#34;value\u0026#34;]) Int = namedtuple(\u0026#34;Int\u0026#34;, [\u0026#34;value\u0026#34;]) Illegal = namedtuple(\u0026#34;Illegal\u0026#34;, []) Eof = namedtuple(\u0026#34;Eof\u0026#34;, []) Assign = namedtuple(\u0026#34;Assign\u0026#34;, []) Bang = namedtuple(\u0026#34;Bang\u0026#34;, []) Dash = namedtuple(\u0026#34;Dash\u0026#34;, []) ... Return = namedtuple(\u0026#34;Return\u0026#34;, []) True_ = namedtuple(\u0026#34;True_\u0026#34;, []) False_ = namedtuple(\u0026#34;False_\u0026#34;, []) Not nearly as pretty, but the advantage is that each member of Token is now a dynamically generated NamedTuple that only has a value attribute if the token type requires it. So identifiers like value in let value = 5; are captured in Token.Ident(\u0026quot;value\u0026quot;), but return tokens (Token.Return()) don\u0026rsquo;t carry around extra baggage.\nWhat\u0026rsquo;s TokenBase, you ask? Well, if you instantiate a member of the Token class without the base class, you\u0026rsquo;ll get something like:\n\u0026gt;\u0026gt;\u0026gt; Token.Ident(\u0026#34;val\u0026#34;) Ident(\u0026#34;val\u0026#34;) \u0026gt;\u0026gt;\u0026gt; Token.Dash() Dash() Close, but not quite as nice as the way that tokens are displayed in the Rust version: Token.Ident(\u0026quot;val\u0026quot;). TokenBase customizes the __repr__s for each of the namedtuples to look more like Rust. (It also modifies __eq__ and __neq__ so that empty namedtuples with different names do not compare equal!) Here\u0026rsquo;s the ugliness to make that work:\nclass TokenBase: def __init_subclass__(cls) -\u0026gt; None: def _token_repr(self): name = self.__class__.__name__ return ( f\u0026#34;Token.{name}({self.value!r})\u0026#34; if hasattr(self, \u0026#34;value\u0026#34;) else f\u0026#34;Token.{name}\u0026#34; ) def _token_eq(self, other): return repr(self) == repr(other) def _token_ne(self, other): return repr(self) != repr(other) for name, method in vars(cls).items(): if name.startswith(\u0026#34;__\u0026#34;): continue method.__repr__ = _token_repr method.__eq__ = _token_eq method.__ne__ = _token_ne The trick is that by using __init_subclass__, the __repr__, __eq__, and __neq__ implementations of the NamedTuple class attributes on Token will be overwritten when the class is imported. This way, Token.Ident(...) will be modified without ever instantiating Token.\nThe end result is that instantiating members of Token will create NamedTuple instances that behave a little bit more like Rust enums.\n\u0026gt;\u0026gt;\u0026gt; Token.Ident(\u0026#34;val\u0026#34;) Token.Ident(\u0026#39;val\u0026#39;) \u0026gt;\u0026gt;\u0026gt; Token.Dash() Token.Dash \u0026gt;\u0026gt;\u0026gt; Token.Dash() != Token.Return() True Shamelessly Copying (Most of) the Lexer Though Rust is a substantially more complex language than Python and has been designed with completely different tradeoffs in mind, both languages provide similar procedural syntax. What follows is a more or less direct port of the Lexer struct and its methods to peek or advance to the next character in the input text.\n@dataclass class Lexer: text: str position: int = 0 read_position: int = 0 ch: str = field(init=False, default=\u0026#34;\u0026#34;) def __post_init__(self): self.read_char() def __iter__(self): while (token := self.next_token()) != Token.Eof(): yield token yield token def next_token(self): ... def peek(self): if self.read_position \u0026lt; len(self.text): return self.text[self.read_position] def read_char(self): if self.read_position \u0026gt;= len(self.text): self.ch = \u0026#34;\u0026#34; else: self.ch = self.text[self.read_position] self.position = self.read_position self.read_position += 1 def skip_whitespace(self): while self.ch.isspace(): self.read_char() def read_ident(self): pos = self.position while self.peek().isalpha() or self.peek() == \u0026#34;_\u0026#34;: self.read_char() return self.input[pos : self.read_position] def read_int(self): pos = self.position while self.peek().isdigit(): self.read_char() return self.input[pos : self.read_position] The Lexer starts at position 0 and steps through the text each time next_token() is called. If the next token contains multiple characters or skippable whitespace, next_token() will advance the Lexer position to the end of the last token. The read_position will then correspond to the start of the next token (or whitespace).\nThe read_ident and read_int functions repeatedly peek at the next character in the input text, instructing the lexer to keep reading the next character until the end of the identifer or integer. The functions record the start and end positions so that they can slice into the input string to return a multi-character token.\nPython makes it easy to create custom iterables, so I added an __iter__ method which will make for token in lexer: print(token) work with no extra sweat!\nMatching match statements Things get interesting again when trying to implement next_token(). The Rust implentation uses a single match statement to identify each of the tokens defined in the lexer. In the python version, matching on individual characters ports directly:\ndef next_token(self): self.skip_whitespace() match self.ch: case \u0026#34;{\u0026#34;: tok = Token.LSquirly() case \u0026#34;}\u0026#34;: tok = Token.RSquirly() case \u0026#34;(\u0026#34;: tok = Token.Lparen() case \u0026#34;)\u0026#34;: tok = Token.Rparen() ... In some cases, such as Token.Bang, and Token.NotEqual, the lexer peeks at the next character to see which token to emit. This ports over nicely as well:\ncase \u0026#34;!\u0026#34;: if self.peek() == \u0026#34;=\u0026#34;: self.read_char() tok = Token.NotEqual() else: tok = Token.Bang() The case for finding identifiers is particularly elegant in the Rust version: the following excerpt matches on any character that contains an alphabetic character or an underscore to find identifiers, and then uses a nested match statement to identify reserved keywords.\nmatch self.ch { ..., b\u0026#39;a\u0026#39;..=b\u0026#39;z\u0026#39; | b\u0026#39;A\u0026#39;..=b\u0026#39;Z\u0026#39; | b\u0026#39;_\u0026#39; =\u0026gt; { let ident = self.read_ident(); return Ok(match ident.as_str() { \u0026#34;fn\u0026#34; =\u0026gt; Token::Function, \u0026#34;let\u0026#34; =\u0026gt; Token::Let, \u0026#34;if\u0026#34; =\u0026gt; Token::If, \u0026#34;false\u0026#34; =\u0026gt; Token::False, \u0026#34;true\u0026#34; =\u0026gt; Token::True, \u0026#34;return\u0026#34; =\u0026gt; Token::Return, \u0026#34;else\u0026#34; =\u0026gt; Token::Else, _ =\u0026gt; Token::Ident(ident), }); }, ... } The ..= is a convenient syntax for creating inclusive ranges that in this case span either the uppercase or lowercase alphabet. Python does not have a direct equivalent, but according to PEP 636, it does have match conditionals! Here\u0026rsquo;s what the equivalent looks like in Python:\ncase t if t.isalpha() or t == \u0026#34;_\u0026#34;: match self.read_ident(): case \u0026#34;fn\u0026#34;: tok = Token.Function() ... case _ as val: tok = Token.Ident(val) case t if t.isdigit(): tok = Token.Int(self.read_int()) Incidentally, this version supports unicode alphabetic characters as well!\nPutting it all together, we get:\ndef next_token(self): self.skip_whitespace() match self.ch: case \u0026#34;{\u0026#34;: tok = Token.LSquirly() case \u0026#34;}\u0026#34;: tok = Token.RSquirly() case \u0026#34;(\u0026#34;: tok = Token.Lparen() case \u0026#34;)\u0026#34;: tok = Token.Rparen() case \u0026#34;,\u0026#34;: tok = Token.Comma() case \u0026#34;;\u0026#34;: tok = Token.Semicolon() case \u0026#34;+\u0026#34;: tok = Token.Plus() case \u0026#34;-\u0026#34;: tok = Token.Dash() case \u0026#34;!\u0026#34;: if self.peek() == \u0026#34;=\u0026#34;: self.read_char() tok = Token.NotEqual() else: tok = Token.Bang() case \u0026#34;\u0026gt;\u0026#34;: tok = Token.GreaterThan() case \u0026#34;\u0026lt;\u0026#34;: tok = Token.LessThan() case \u0026#34;*\u0026#34;: tok = Token.Asterisk() case \u0026#34;/\u0026#34;: tok = Token.ForwardSlash() case \u0026#34;=\u0026#34;: if self.peek() == \u0026#34;=\u0026#34;: self.read_char() tok = Token.Equal() else: tok = Token.Assign() case t if t.isalpha() or t == \u0026#34;_\u0026#34;: match self.read_ident(): case \u0026#34;fn\u0026#34;: tok = Token.Function() case \u0026#34;let\u0026#34;: tok = Token.Let() case \u0026#34;if\u0026#34;: tok = Token.If() case \u0026#34;false\u0026#34;: tok = Token.False_() case \u0026#34;true\u0026#34;: tok = Token.True_() case \u0026#34;return\u0026#34;: tok = Token.Return() case \u0026#34;else\u0026#34;: tok = Token.Else() case _ as val: tok = Token.Ident(val) case t if t.isdigit(): tok = Token.Int(self.read_int()) case \u0026#34;\u0026#34;: tok = Token.Eof() case _: tok = Token.Illegal self.read_char() return tok Not bad!\nChecking our work The last task is to reimplement the tests. Here\u0026rsquo;s the smaller of the two tests in the reference, taking advantage of the __iter__ implementation that we added to the lexer:\ndef test_next_small(): text = \u0026#34;=+(){},;\u0026#34; lexer = Lexer(text) tokens = [ Token.Assign(), Token.Plus(), Token.Lparen(), Token.Rparen(), Token.LSquirly(), Token.RSquirly(), Token.Comma(), Token.Semicolon(), ] for exp, res in zip(tokens, lexer): print(f\u0026#34;expected: {exp}, received {res}\u0026#34;) assert exp == res \u0026gt;\u0026gt;\u0026gt; test_next_small() expected: Token.Assign, received Token.Assign expected: Token.Plus, received Token.Plus expected: Token.Lparen, received Token.Lparen expected: Token.Rparen, received Token.Rparen expected: Token.LSquirly, received Token.LSquirly expected: Token.RSquirly, received Token.RSquirly expected: Token.Comma, received Token.Comma expected: Token.Semicolon, received Token.Semicolon As usual, the full source for this post is available here.\nMany contributors submitted equivalent lexers in a variety of languages for comparison, including one in python that even includes a parser.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://willduke.github.io/posts/06-lexer/","summary":"A couple of months ago, I came across a video where the prolific streamer and Netflix dev known online as the ThePrimeagen live-coded a simple lexer for an interpreter inspired by the book Crafting Interpreters by Robert Nystrom. If you\u0026rsquo;ve ever looked into what CPython actually does with your source code, you\u0026rsquo;ll know that it parses the code into an abstract syntax tree, converts the tree into bytecode instructions (i.e. those *.","title":"A Poorman's Rust Enum in Python for ThePrimeagen's Lexer"},{"content":"Pandas is a fantastic library for data analysis, but its syntax can be a bit jarring to the unfamiliar user, especially one coming from the R tidyverse ecosystem where the %\u0026gt;% (pipe) operator makes method-chaining powerful and preferred for most operations. It turns out that a similar syntax is totally possible in pandas with the pipe and assign methods! With these, you can make your pandas code much more readable and reusable.\nFrom the pandas docs, the pipe method lets you write:\n(df.pipe(h) .pipe(g, arg1=a) .pipe(func, arg2=b, arg3=c) ) instead of:\nfunc(g(h(df), arg1=a), arg2=b, arg3=c) By default, pipe passes the dataframe to the first argument of the function along with any specified keyword arguments. This lets you write any function that receives and returns a dataframe and add it into a series of pipes.\nLet\u0026rsquo;s dive into an example.\nFrequently, data that was entered by hand has small variations (typos, style changes, etc.) that need to be ironed out before the data can be used for modeling. In my toy example based on some real data I\u0026rsquo;ve received below, I\u0026rsquo;d like to predict themes from service reviews that were labeled by hand.\nimport pandas as pd raw = pd.DataFrame( { \u0026#39;id\u0026#39;: [\u0026#39;001\u0026#39;, \u0026#39;002\u0026#39;, \u0026#39;003\u0026#39;, \u0026#39;004\u0026#39;], \u0026#39;label\u0026#39;: [ \u0026#39;great Communication\u0026#39;, \u0026#39;attention to detail\u0026#39;, \u0026#39;great communication \u0026#39;, \u0026#39;Attention to Detail\u0026#39; ] } ) As you can see, the labels are mostly right barring some issues with extraneous spaces and varying capitalization. As a first pass, I could write the following code to standardize the themes to snake case:\nraw[\u0026#39;label\u0026#39;] = ( raw[\u0026#39;label\u0026#39;] .str.strip() .str.lower() .str.replace(r\u0026#39;\\W+\u0026#39;, \u0026#39;_\u0026#39;, regex=True) ) This code works, but it\u0026rsquo;s tightly coupled to the column name, which is written in two places. It\u0026rsquo;s also not immediately obvious why I\u0026rsquo;m going to the trouble without a comment or prior knowledge of the data. Better to wrap it in a function!\ndef convert_to_snake_case( dataframe: pd.DataFrame, columns: List[str] ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34;Convert values in string columns in a dataframe to snake case, handling extraneous spacing and capitalization. \u0026#34;\u0026#34;\u0026#34; assert set(columns).issubset( set(dataframe.columns) ), \u0026#34;At least one column is missing.\u0026#34; assert all( pd.api.types.is_string_dtype(dataframe[column]) for column in columns ), \u0026#34;All columns must be of string dtype.\u0026#34; return dataframe.assign( **{ column: ( dataframe[column] .str.strip() .str.lower() .str.replace(r\u0026#39;\\W+\u0026#39;, \u0026#39;_\u0026#39;, regex=True) ) for column in columns } ) I can avoid using the bracket syntax to modify columns by passing keyword arguments (here, using the ** expansion syntax with a dictionary comprehension) to assign to redefine all of the columns. The assign method returns a dataframe, so could I chain any dataframe method after it. Since I\u0026rsquo;ve specified that this function takes a DataFrame as its first argument, it can be easily used in a pipe.1\ntabs = ( old .pipe(convert_to_snake_case, columns = [\u0026#39;label\u0026#39;]) .value_counts() ) One caveat to this approach is that the tracebacks resulting from errors occuring in the piped function can be a bit more difficult to parse since the proximate cause of the error will be pandas error-handling code instead of the function call. For this reason, I like to include asserts that guard against common mistakes in the functions that I might pipe to make them easier to debug. Hence, the assert calls above checking that every column in columns exists and has a string data type.\nExtracting data preprocessing components into functions has a nice payoff when combined in a series of pipes. For example, you might structure your code like this:\ndef load_raw() -\u0026gt; pd.DataFrame: return pd.read_csv(...) # e.g. def remove_invalid_columns(data: pd.DataFrame) -\u0026gt; pd.DataFrame: ... def remove_invalid_rows(data: pd.DataFrame) -\u0026gt; pd.DataFrame: ... def convert_to_snake_case(data: pd.DataFrame, columns: List[str]) -\u0026gt; pd.DataFrame: ... def run_all(data: pd.DataFrame) -\u0026gt; pd.DataFrame: return ( .pipe(self.remove_invalid_columns) .pipe(self.remove_invalid_rows) .pipe(self.convert_to_snake_case, columns = [\u0026#34;label\u0026#34;]) ) Of course, all of this is possible with nested function calls or intermediate variables in the absence of pipe, but I find the piped functions above easier to follow without the visual noise. Plus, it\u0026rsquo;s easier to take the time to reorganize data munging code into component functions when the final run_all function looks so nice!\nYou can pass a (function, dataframe-argument-name) tuple as the first argument to pipe if the dataframe argument is later in the function signature.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://willduke.github.io/posts/05-pretty-pipes/","summary":"Pandas is a fantastic library for data analysis, but its syntax can be a bit jarring to the unfamiliar user, especially one coming from the R tidyverse ecosystem where the %\u0026gt;% (pipe) operator makes method-chaining powerful and preferred for most operations. It turns out that a similar syntax is totally possible in pandas with the pipe and assign methods! With these, you can make your pandas code much more readable and reusable.","title":"Pretty Pipes for Perusable (and Reusable) Pandas Procedures"},{"content":"A few weeks ago, Matt Parker released a video where he posed a question he\u0026rsquo;d received from a viewer about the popular game Wordle: how many words can you try in a Wordle game without reusing a letter? At most, the answer can be 5 (25 letters total with one unused). So the question becomes: are there sets of five English words without any overlapping letters, and (while we\u0026rsquo;re at it) if so, how many?\nTo answer this, Parker used a relatively naive approach that took about a month to run on an extra laptop. Quite a few clever people have since optimized their solutions to run much faster. In particular, Parker provided a link to Benjamin Paasen\u0026rsquo;s method, which used graph theory to bring the running time down to just over 20 minutes.\nEssentially, the problem of finding sets of non-overlapping words can be reframed as finding a fully-connected subgraph in a network where all nodes are words and edges connect words which share no letters. By fully-connected, I mean that every node (word) has an edge (i.e. shares no letters) with every other word in that part of the network. In graph theory, these groups are known as cliques and there are a number of algorithms for finding them.\nPassen implemented a method that builds cliques by searching neighbors for intersections of progressively more words. I thought we\u0026rsquo;d give the same approach a try, but see if we can take advantage of Python\u0026rsquo;s fantastic ecosystem \u0026ndash; namely, the igraph package \u0026ndash; to further improve the speed. First, we\u0026rsquo;ll need to recreate the set of words from which to search for five-word cliques. I used Parker\u0026rsquo;s source for all English words: the words_alpha.zip file which can be found here.\nPreparing the Word List in (Functional) Style After downloading the zip archive, we can load the words as an iterable using the zipfile library:\nimport zipfile from pathlib import Path from typing import Dict, Iterable, List, Tuple def extract_archive_to_word_list( archive_path: Path, filename: str ) -\u0026gt; Iterable[str]: \u0026#34;\u0026#34;\u0026#34; Provided a path to a zip archive and the name of the file within the archive containing an english word list, extract and read the file and return an iterable of the words. \u0026#34;\u0026#34;\u0026#34; return ( zipfile.ZipFile(archive_path, \u0026#39;r\u0026#39;) .read(filename) .decode(\u0026#39;utf-8\u0026#39;) .split(\u0026#34;\\r\\n\u0026#34;) ) Now, we have a series of filtering operations to do to select only 5 letter words without duplicate letters and to keep only one word from word sets which contain the same letters. I\u0026rsquo;ll use the pipe package to make each function work with shell-style piping to avoid creating intermediate variables.\nfrom pipe import Pipe @Pipe def filter_words_of_length_n(words: List[str], n: int) -\u0026gt; Iterable[str]: \u0026#34;\u0026#34;\u0026#34;Filter out words not of length n.\u0026#34;\u0026#34;\u0026#34; return filter(lambda word: len(word) == n, words) @Pipe def filter_words_with_duplicate_letters(words) -\u0026gt; Iterable[str]: \u0026#34;Filter out words with more than one of any letter.\u0026#34; return filter(lambda word: len(word) == len(set(word)), words) @Pipe def filter_duplicate_word_sets(words: Iterable[str]) -\u0026gt; Iterable[str]: \u0026#34;\u0026#34;\u0026#34;Filter out words with the same set of letters as an already seen word.\u0026#34;\u0026#34;\u0026#34; seen = set() seen_add = seen.add for word in words: wordset = frozenset(word) if wordset not in seen: seen_add(wordset) yield word By filtering our word list with each of these functions, we can keep only words of length 5 which have no duplicate letters and remove any words that share all letters with words previously seen in the list. (For example, \u0026ldquo;create\u0026rdquo; and \u0026ldquo;react\u0026rdquo; share the same letters, so we\u0026rsquo;ll just choose one to include since the other will share all of the same cliques.) Thanks to the @Pipe decoration, each of these operations can be chained together with the | operator (which calls the next function on the result of the previous one):\n@Pipe def get_unique_set_words_of_length_n( words: Iterable[str], n: int ) -\u0026gt; Iterable[str]: \u0026#34;\u0026#34;\u0026#34;Get the filtered list of words of length n with no repeating digits, omitting any words with duplicate letter sets.\u0026#34;\u0026#34;\u0026#34; return ( words | filter_words_of_length_n(n) | filter_words_with_duplicate_letters | filter_duplicate_word_sets ) Note that the @Pipe decorator curries filter_words_of_length_n so that it can be called once for each of its arguments. The first argument is presumed to be the iterable, so calling the function with n=5 returns a partial function with the n parameter pre-filled.\nOk, with that done, we can move on to creating our graph.\nBuilding a Graph with iGraph The igraph package provides a powerful Python interface to its optimized-C core graph routines, which we can use to find our 5-cliques. Creating an igraph.Graph object requires a list of all of the edges in the graph identified by tuples of vertex indices. (The constructor will infer the nodes from this list.)\nimport itertools as it from igraph import Graph def create_graph_of_disjoint_words(words: Iterable[str]) -\u0026gt; Graph: \u0026#34;\u0026#34;\u0026#34; Create a igraph.Graph where each vertex is a word and two words share an edge if they have no letters in common. igraph.Graph takes a list of edges as tuples of vertex indices. The edges are created by iterating through each word and adding edges for any remaining words that have disjoint letter sets. \u0026#34;\u0026#34;\u0026#34; wordsets = list(map(set, words)) edges = [ (i, j) for i, left in enumerate(wordsets) for j, right in it.islice(enumerate(wordsets), i+1, None) if left.isdisjoint(right) ] return Graph(edges = edges) To create our list of vertices, we iterate through each of the words and add a tuple of node IDs (indices corresponding to the each word\u0026rsquo;s position in the word list) where the letter sets of the two words are disjoint (i.e. they don\u0026rsquo;t share any letters). Starting from the beginning of the word list, we check only the remaining words in the list for disjoint pairs each time to avoid rechecking the pairs of words (in reverse order).\nPassing this edge list to igraph.Graph instantiates a graph with a node for each of the words in our word list with edges between words with disjoint sets of letters.\nFinding all the Cliques igraph.Graph has a cliques() method which will return cliques between a minimum and maximum size. Under the hood, this method calls the C method igraph_cliques(), which makes use of the Cliquer library written largely by Patric Östergård.1 For our purposes, finding cliques of a certain size is as simple as specifying the same lower and upper bound:\ndef find_all_size_n_cliques( words: Iterable[str], size: int ) -\u0026gt; Iterable[Dict[int, Tuple[str, ...]]]: \u0026#34;\u0026#34;\u0026#34;Provided an iterable of strings, return all of the sets of words of a given size with no overlapping letters between any pair in the set.\u0026#34;\u0026#34;\u0026#34; _words = list(words) graph = create_graph_of_disjoint_words(_words) for pos, clique in enumerate(graph.cliques(size, size)): yield {pos: tuple(_words[idx] for idx in clique)} Note: Here, we call list() on the words iterable since we\u0026rsquo;ll need it multiple times. (Otherwise, the iterable may be exhausted if it is a generator.)\nI like using jsonlines as a serialization format, so I\u0026rsquo;ve formatted each set of words as a dictionary with the clique index as the key and the tuple of words as the values. (The cliques algorithm returns the node indices, so we have to recover the original words).\nLastly, we can write can create a main function that will write the results of this search to a jsonl file.\nPARENT_DIR = Path(__file__).parent WORDS_ARCHIVE_PATH = PARENT_DIR / \u0026#34;words_alpha.zip\u0026#34; WORDS_FILENAME = \u0026#34;words_alpha.txt\u0026#34; CLIQUES_PATH = PARENT_DIR / \u0026#34;cliques.jsonl\u0026#34; def main( words_archive_path: Path = WORDS_ARCHIVE_PATH, words_filename: str = WORDS_FILENAME, cliques_path: Path = CLIQUES_PATH ): \u0026#34;\u0026#34;\u0026#34; Load a list of words from a file (filename: word_filename) within a zip archive (words_archive_path). Save a file with all of the cliques (of mutually exlusive letters) to cliques_path. \u0026#34;\u0026#34;\u0026#34; words = ( extract_archive_to_word_list(words_archive_path, words_filename) | get_unique_set_words_of_length_n(5) ) five_cliques = find_all_size_n_cliques(words, 5) with jsonlines.open(cliques_path, \u0026#39;w\u0026#39;) as writer: writer.write_all(five_cliques) if __name__ == \u0026#39;__main__\u0026#39;: main() Including the time to load in the data and construct the igraph.Graph, this script took 10.2 seconds on my 2019 Macbook Pro. In total, there were 538 five-word sets using 25 letters (ignoring anagrams).\nHere\u0026rsquo;s a sampling of some of the word sets:\nchivw enzym fldxt jakob sprug ampyx crwth fdubs glink vejoz bortz chivw dunks flegm japyx Maybe I shouldn\u0026rsquo;t be surprised that these aren\u0026rsquo;t quite in the vernacular.\nYou can find the complete source for this post here.\nThe source for Cliquer\u0026rsquo;s C implementation as vendored in iGraph can be found here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://willduke.github.io/posts/04-word-cliques/","summary":"A few weeks ago, Matt Parker released a video where he posed a question he\u0026rsquo;d received from a viewer about the popular game Wordle: how many words can you try in a Wordle game without reusing a letter? At most, the answer can be 5 (25 letters total with one unused). So the question becomes: are there sets of five English words without any overlapping letters, and (while we\u0026rsquo;re at it) if so, how many?","title":"Solving Matt Parker's Wordle Puzzle in Seconds with iGraph"},{"content":"Notebooks are exceptionally popular among data scientists. So much so that many of my colleagues work largely in JupyterHub or Sagemaker. Notebooks provide a convenient enviroment for quick prototyping, and they feel natural for data scientists that (like me) don\u0026rsquo;t have a formal computer science background. That said, I\u0026rsquo;m convinced that we are using them in many cases where they aren\u0026rsquo;t worth the downsides, and even that the popular wisdom that they are a good place to get started quickly has it quite backwards.\nWell, let me first make sure to give notebooks a fair shake. There are a ton of cool things you can do with notebooks that I think are great:\nAvoiding (initial) packaging headaches with a pre-configured environment is a huge boon to data scientists who don\u0026rsquo;t want to deal with virtual environments and dependencies when doing one-off analyses. Writing narrative-driven code with the literate style of programming made possible with code and markdown cells plus inline output makes for great reports. Developing visualizations is so natural in a notebook when you want to iteratively refine a plot or series of plots for your eventual output. (Data scientists love this because it reminds us of the dedicated plot window in RStudio\u0026hellip;Spyder, anyone?). Demoing code for others can be easy and expressive in a notebook, when you can prepare each cell to return intermediate results and walk someone else through your code. Fancying up your documentation using tools like nbsphinx which render notebooks right into your Sphinx docs, helping others get started with your project. All that aside, here are some downsides to watch out for:\nIt\u0026rsquo;s Hard to Make Code Modular Ideally, the source code for a project should be broken into logical pieces \u0026ndash; functions, classes, and modules contributing distinct features to a project. This value is built into many programming tools: Linters now warn on long methods, and some projects even reject commits on the basis of cyclomatic complexity via CI checks.\nBut fundamentally, the pattern of working in a notebook makes this process of decomposition unnatural. Nothing queues the user that there is a difference between definition and execution code, so function definitions often get mixed in with parts of the analysis. This makes it difficult for someone reading the code to understand the context of its execution.\nWhat\u0026rsquo;s more, the individual cells feel a little like functions themselves, which makes it easy to write a lot of top-level code that is then run almost like a script to load or modify an object that is used elsewhere. I think this contributes to the state of many notebooks I see which have almost no functions defined.\nThese patterns are forgivable given how unwieldy refactoring is in a notebook. Copying code between cells involves a lot of scrolling \u0026ndash; and all too quickly the notebook gets messy and large enough that most of the relevant code is not visible at the same time. At least for me, I need to be looking at the code as I reason about it; otherwise, I seem to get caught in an endless loop of scrolling up to remember exactly how that helper method was defined, scrolling down to use it, making an error, and scrolling up again.\nIt\u0026rsquo;s Tough to Handle Implicit State Once a notebook gets a little dirty, it becomes easy to get into a state that is difficult to reproduce. I seem to find it both incredibly tempting to run cells out of order, and then incredibly difficult to reconstruct the state of the notebook later. I think to myself,\nOh, I\u0026rsquo;ll just fix that bug in an earlier cell and re-run it. (repeats a few times)\nOK, I think I\u0026rsquo;m in a good place for the day (shuts down kernel)\nchaos ensues\nIt\u0026rsquo;s far too easy to create circumstances where code that appears to be perfectly valid raises an error because of some unusual hidden state.\nThe ability to freely run cells out of order, while convenient, breaks our convenant with the machine that - barring the explicit introduction of concurrency \u0026ndash; things will happen (or have happened) in a predictable linear order. I can look at my screen and see two cells that define a logical set of steps and then get a confusing error on running the latter cell because a variable was silenty overwritten when I ran a third cell I thought was unrelated.1\nReproducing Results can be Tricky The output of a notebook depends on the kernel that was used to run it, and different individuals may not have access to or have set up the same environment. Sometimes, the notebooks themselves contain pip magic that install new packages or makes other environment changes. This can make it very difficult to reproduce the set of packages that were used to generate the results. Notebooks give the appearance of being standalone files, so they are frequently exchanged without bringing along enough information to reproduce the initial kernel state.\nTooling is Limited Notebooks don\u0026rsquo;t have great support for many useful productivity tools:\nNotebooks accessed through a web interface lack autocomplete, built-in rollover suggestions, method extraction, debugging, etc. Without workarounds, diffs in source control are essentially useless Testing frameworks, linters, autoformatters, and other code quality tools have limited support Without these tools, it\u0026rsquo;s much more difficult to maintain code style and quality standards and maintain confidence that code is working as intended.\nDeploying Eventually, we\u0026rsquo;d like to deploy our models \u0026ndash; at which point we\u0026rsquo;ll want all of the extra tooling and a complete description of our dependencies anyway!\nWrapping Up The trouble with notebooks that don\u0026rsquo;t have separate definition and execution code, modularity, or a necessarily linear execution order is, put simply, that they usually don\u0026rsquo;t run.\nHow many times have you received a notebook of any appreciable length, hit run all, and made it through to the last cell?\nIt\u0026rsquo;s happened a few blessed times for me, but mostly I\u0026rsquo;m quickly swearing, trying to figure out what went wrong.\nI think that notebooks impose such a cost on reproducibility, collaboration, and even code correctness that they are just not worth the convenience outside demos, informal experimenting, and results sharing.\nFor especially good examples of this phenomenon and more issues with notebooks, check out Joel Grus\u0026rsquo; entertaining talk I Don\u0026rsquo;t Like Notebooks. The memes alone make it worth a watch.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://willduke.github.io/posts/03-notebooks/","summary":"Notebooks are exceptionally popular among data scientists. So much so that many of my colleagues work largely in JupyterHub or Sagemaker. Notebooks provide a convenient enviroment for quick prototyping, and they feel natural for data scientists that (like me) don\u0026rsquo;t have a formal computer science background. That said, I\u0026rsquo;m convinced that we are using them in many cases where they aren\u0026rsquo;t worth the downsides, and even that the popular wisdom that they are a good place to get started quickly has it quite backwards.","title":"The Trouble with Notebooks"},{"content":"Awhile back, I came across a video from one of my favorite channels, Numberphile, on Conway\u0026rsquo;s Game of Life. Turns out, it\u0026rsquo;s not a game you can actually \u0026ldquo;play,\u0026rdquo; beyond setting the initial state of a simulation that progresses via a set of very simple rules. Devised by the British mathematician John Conway in 1970, the game is composed of a grid of cells, each of which can be alive or dead. Each cell\u0026rsquo;s state in the next generation depends on that of its neighbors, according to the following set of rules:\nAny live cell with fewer than two live neighbours dies, as if by underpopulation. Any live cell with two or three live neighbours lives on to the next generation. Any live cell with more than three live neighbours dies, as if by overpopulation. Any dead cell with exactly three live neighbours becomes a live cell, as if by reproduction. \u0026ndash; Wikipedia\nLet\u0026rsquo;s implement this in numpy and scipy, the foundational packages in Python\u0026rsquo;s scientific computing ecosystem.\nCreating a random grid with numpy A grid of cells that can be alive (1) or dead (0) is just a matrix of binary values. Using numpy\u0026rsquo;s utilities for generating random arrays, we can create an example grid of random ones and zeroes like this:\n\u0026gt;\u0026gt;\u0026gt; import numpy as np \u0026gt;\u0026gt;\u0026gt; def create_random_grid(length: int = 4, width: int = 4, seed = None) -\u0026gt; np.array: ... \u0026#34;\u0026#34;\u0026#34;Create a random 2D binary array sampled from a uniform distribution.\u0026#34;\u0026#34;\u0026#34; ... rng = default_rng(seed = seed) ... return rng.integers( ... low = 0, high = 2, size = length * width ... ).reshape(length, width) \u0026gt;\u0026gt;\u0026gt; grid = create_random_grid() \u0026gt;\u0026gt;\u0026gt; print(grid) array([[0, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 1], [1, 0, 1, 1]]) In this code block, we use numpy\u0026rsquo;s default random number generator (RNG) to create an array of 16 integers in the range [0, 2) (the upper bound is excluded), which we then reshape into a 4x4 matrix. This will represent the initial conditions of Conway\u0026rsquo;s game.\nCounting neighbors with scipy\u0026rsquo;s ndimage library Next, we need a way to check the number of living neighbors of each cell. In an infinite grid, every cell has 8 neighbors: 3 above, 3 below, and one on either side. Including the central cell, that\u0026rsquo;s a 3x3 window that we\u0026rsquo;ll need to construct for every cell in the grid, counting the live cells each time. We can then set the new value of the central cell based on the sum of its neighbors. If this sounds familiar, it might be because this is the same kind of procedure you would see in a layer of a convolutional neural network, where a kernel is applied over a sliding window of an image or signal. We don\u0026rsquo;t need to install pytorch for our game though!\nInstead, we\u0026rsquo;ll use the convolve function in scipy.ndimage 1. Provided our initial grid and a kernel, convolve will perform an element-wise multiplication of the elements in the window and the kernel and then sum the result for every cell in the grid. For the mathematically inclined, the docs describe it thus,\n[Each element in the convolved array is] $C_i = \\sum_{j} I_{i+k-j}W_j$ where W is the weights kernel, j is the N-D spatial index over W, I is the input and k is the coordinate of the center of W, specified by origin in the input parameters.\n\u0026ndash; scipy\u0026rsquo;s excellent API reference\nSince our grid is a binary matrix, convolving it with a 3x3 kernel of all ones will count the number of live cells in the window. If we change the central value of the kernel to a zero, we\u0026rsquo;ll exclude the central cell and just count the neighbors.\n\u0026gt;\u0026gt;\u0026gt; from scipy import ndimage \u0026gt;\u0026gt;\u0026gt; def count_neighbors(grid: np.array) -\u0026gt; np.array: ... \u0026#34;\u0026#34;\u0026#34;Count the live neighbors of each cell in a 2D binary array, treating off-grid neighbors as dead. Returns an np.array of the same size as the input.\u0026#34;\u0026#34;\u0026#34; ... kernel = np.array([1,1,1,1,0,1,1,1,1]).reshape(3,3) ... return ndimage.convolve(grid, kernel, mode = \u0026#39;constant\u0026#39;, cval = 0.0) \u0026gt;\u0026gt;\u0026gt; neighbors = count_neighbors(grid) \u0026gt;\u0026gt;\u0026gt; print(neighbors) array([[2, 3, 3, 2], [3, 5, 7, 4], [4, 6, 6, 4], [2, 5, 4, 3]]) Here, setting the mode to constant with a cval of 0.0 instructs convolve to treat any off-grid neighbors (from cells on the edges) as 0.0 by padding the outside of the grid2.\nNow, for the rules.\nApplying the rules Rather than jump in and implement each of the 4 rules above, let\u0026rsquo;s consider the possible cases where a cell could be alive in the next generation.\nAccording to rule (2), a cell that is alive and has 2 neighbors will stay alive. Based on rules (2) and (4), any cell, alive or dead, with 3 neighbors will be alive. Factoring in rules (1) and (3), we know every other cell will die or remain dead. Taking advantage of numpy\u0026rsquo;s built-in, vectorized methods for element-wise comparisons, we can use boolean logic to implement these rules:\n\u0026gt;\u0026gt;\u0026gt; def apply_conways_rules(grid: np.array) -\u0026gt; np.array: ... \u0026#34;\u0026#34;\u0026#34;Provided a 2D array of neighbor counts, return the next generation of living cells.\u0026#34;\u0026#34;\u0026#34; ... neighbors = count_neighbors(grid) ... return ((neighbors == 2) \u0026amp; grid) | (neighbors == 3) \u0026gt;\u0026gt;\u0026gt; apply_conways_rules(grid) array([[0, 1, 1, 1], [1, 0, 0, 0], [0, 0, 0, 0], [1, 0, 0, 1]]) As in plain Python, 1 is \u0026ldquo;truthy\u0026rdquo; and 0 is \u0026ldquo;falsy\u0026rdquo; so we can smoothly convert between them. The \u0026amp; operator is equivalent to a logical and operation, and the | operator performs a logical or3. So, in effect, we check for each cell whether it has 2 neighbors and is alive or if it has exactly 3 neighbors. The logical operators return integers rather than booleans, so we get back our new grid in one line!\nCreating an infinite simulator Now, we just need a little scaffolding to repeatedly yield the next generation of the simulation.\n\u0026gt;\u0026gt;\u0026gt; from typing import Generator \u0026gt;\u0026gt;\u0026gt; def simulate(grid: np.array) -\u0026gt; Generator[np.array, None, None]: ... \u0026#34;\u0026#34;\u0026#34;Yield infinite generations of Conway\u0026#39;s game provided a starting grid.\u0026#34;\u0026#34;\u0026#34; ... ... while True: ... grid = apply_conways_rules(grid) ... yield grid Provided an initial grid, simulate will return a generator, which will lazily compute the next generation(s) when we ask for it. For example, we can use the islice function from the itertools package to return the state of the simulation after 5 rounds.\n\u0026gt;\u0026gt;\u0026gt; import itertools as it \u0026gt;\u0026gt;\u0026gt; generator = simulate(grid) \u0026gt;\u0026gt;\u0026gt; next(it.islice(generator, 5, None)) array([[0, 0, 1, 1], [0, 1, 0, 1], [1, 1, 0, 1], [0, 0, 1, 1]]) Visualizing with matplotlib Lastly, let\u0026rsquo;s create a visualization of the evolving grid. We\u0026rsquo;ll need two dependencies for this: matplotlib and ffmpeg4. matplotlib provides an animation module, which we can use to create animations that we\u0026rsquo;ll save to a gif with ffmpeg5. We\u0026rsquo;ll use matplotlib.animation.FuncAnimation to create our gif6.\nThe process is as follows:\nUse islice to collect a list of matrices representing consecutive generations of the simulation Create a matplotlib.pyplot figure with plt.matshow of the initial grid Define init and update functions which will repeatedly update the data in the plot to create the animation Create the animation with matplotlib.animation.FuncAnimation Save the animation as a gif with matplotlib.animation.FFMpegWriter \u0026gt;\u0026gt;\u0026gt; def save_conway_to_gif(grid: np.array, filepath: PathLike, nframes: int = 100) -\u0026gt; None: ... \u0026#34;\u0026#34;\u0026#34; Create a gif of the first nframes of evolution of the provided grid and save it to filepath. \u0026#34;\u0026#34;\u0026#34; ... # create a list of arrays representing the first NFRAMES generations ... data = list(it.islice(simulate(grid), 0, nframes)) ... # create an initial matrix figure without ticks or labels ... fig = plt.figure() ... plot = plt.matshow(data[0], fignum=0, cmap = plt.get_cmap(\u0026#39;binary\u0026#39;)) ... plt.tick_params(left = False, right = False , labelleft = False, ... labelbottom = False, bottom = False, top = False, ... labeltop = False) ... def init(): ... \u0026#34;\u0026#34;\u0026#34; Provides the initial plot (to be passed to FuncAnimation\u0026#39;s init_func arg as a callable) \u0026#34;\u0026#34;\u0026#34; ... plot.set_data(data[0]) ... return [plot] ... def update(j): ... \u0026#34;\u0026#34;\u0026#34; Updates the figure\u0026#39;s data in place (to be passed to the FunAnimation func arg as a callable). \u0026#34;\u0026#34;\u0026#34; ... plot.set_data(data[j]) ... return [plot] ... metadata = dict( ... title=\u0026#34;Conway\u0026#39;s Game of Life\u0026#34;, artist=\u0026#39;Will\u0026#39;, comment=\u0026#39;\u0026#39; ... ) ... writer = animation.FFMpegWriter(fps=5, metadata=metadata, bitrate=3500) ... anim = animation.FuncAnimation( ... fig, update, init_func = init, ... frames=nframes, interval = 30, blit=True ... ) ... anim.save(filepath, writer = writer, dpi = 300) If we use the create_random_grid function to select a random initial state of the simulation (where each cell has equal probability to be alive or dead), we might see something like this:\nAlready some interesting, periodic patterns!\nLastly, I took a moment to get the plaintext code for a cool, repeating pattern, the Gosper glider gun. This pattern repeatedly emits new \u0026lsquo;spaceships\u0026rsquo; and cycles back to its initial state every 30 generations.\nI\u0026rsquo;m barely scratching the surface of the interesting patterns that have been discovered. If you\u0026rsquo;re interested in going down the rabbit hole, I\u0026rsquo;d recommend conwaylife.com (the homepage of the wiki is quite a trip!).\nYou can find the complete source for this post here.\nFor a purely numpy based implementation, we can use the sliding_window_view function in numpy\u0026rsquo;s stride_tricks module. First pad the grid with zeroes using np.pad, apply a 3x3 window with sliding_window_view, sum across the 2nd and 3rd axes, and subtract the original grid to exclude the central cell from each count.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor image processing, we\u0026rsquo;d probably want to extend out the color at the ends (as though the same pixel color continued outside the edge of the image). The default value for the mode parameter, reflect, does exactly that.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf functions are needed rather than infix operators, numpy provides logical_and and logical_or (as well as logical_not and logical_xor).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nffmpeg the python package is a wrapper around ffmpeg the command line tool, which requires a separate installation. I installed it on my Mac with homebrew: brew install ffmpeg.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe matplotlib.animation library also supports imagemagick and pillow.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nUsing the FuncAnimation class can be a bit unwieldy, since the init_func and func must keep references to an external plot and update the data in place. Also tricky is that the class expects a sequence of plots (or artists, as the docs refers to them) at each iteration, not the updated plot as you might expect.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://willduke.github.io/posts/02-conway-with-numpy/","summary":"Awhile back, I came across a video from one of my favorite channels, Numberphile, on Conway\u0026rsquo;s Game of Life. Turns out, it\u0026rsquo;s not a game you can actually \u0026ldquo;play,\u0026rdquo; beyond setting the initial state of a simulation that progresses via a set of very simple rules. Devised by the British mathematician John Conway in 1970, the game is composed of a grid of cells, each of which can be alive or dead.","title":"Conway's Game of Life in Just a Few Lines with Python's Scientific Stack"},{"content":"This is my first blog post using Hugo and the Hyde theme. Looks nice!\n","permalink":"https://willduke.github.io/posts/01-my-first-post/","summary":"This is my first blog post using Hugo and the Hyde theme. Looks nice!","title":"My First Post"}]